{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fez2KEiqT0PO"
      },
      "source": [
        "Notebook prepared by Mathieu Blondel and Pierre Ablin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP2BK_aX_I2-"
      },
      "source": [
        "# Lecture 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuAYuQ95reai"
      },
      "source": [
        "## Ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3SYMuhGT_1Ok"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California housing dataset.\n",
        "X_california, y_california = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split into 60% training, 20% validation and 20% test.\n",
        "X_california_tr, X_rest, y_california_tr, y_rest = \\\n",
        "  train_test_split(X_california, y_california, test_size=0.4, random_state=0)\n",
        "X_california_val, X_california_te, y_california_val, y_california_te = \\\n",
        "  train_test_split(X_rest, y_rest, test_size=0.5, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtoOJ8agAd95"
      },
      "source": [
        "**Exercise 1.** Implement the analytical solution of ridge regression $(X^\\top X + \\alpha I) w = X^\\top y$ (see [slides](https://data-psl.github.io/lectures2022/slides/05_optimization_linear_models)) using [scipy.linalg.solve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html). Compute the solution on the training data. Make sure that the gradient at the solution is zero (up to machine precision)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heG7kuDXB3Md"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import solve\n",
        "\n",
        "def ridge_regression_solution(X, y, alpha):\n",
        "  # Write your function here\n",
        "  return w_star\n",
        "\n",
        "def ridge_regression_gradient(w, X, y, alpha):\n",
        "  # Write your function here\n",
        "  return gradient\n",
        "\n",
        "w_star = ridge_regression_solution(X_california_tr, y_california_tr, alpha=1)\n",
        "\n",
        "gradient = ridge_regression_gradient(w_star, X_california_tr, y_california_tr, alpha=1)\n",
        "np.sqrt(np.sum(gradient ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4wef-U2CGVX"
      },
      "source": [
        "**Exercise 2.** Train the models for several possible values of alpha (see below). Plot the mean squared error on the test set as a function of alpha. Use the validation data to find the best alpha and display it on the graph using a circle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWDX9MAfC_5I"
      },
      "outputs": [],
      "source": [
        "alphas = np.logspace(-10, 10, 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9fAF9FDAVO"
      },
      "source": [
        "**Bonus exercise.** Implement a scikit-learn compatible estimator class (with fit and predict methods). Compare that you obtain the same results as `sklearn.linear_model.Ridge(fit_intercept=False)`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "class MyRidge(BaseEstimator, RegressorMixin):\n",
        "  def __init__(self, alpha=1.0):\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    # Write your code here\n",
        "    return self\n",
        "\n",
        "  def predict(self, X):\n",
        "    # Write your code here\n",
        "    return\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "print(MyRidge().fit(X_california_tr, y_california_tr).predict(X_california_te)[:10])\n",
        "print(Ridge(fit_intercept=False).fit(X_california_tr, y_california_tr).predict(X_california_te)[:10])"
      ],
      "metadata": {
        "id": "K4UBYvhyKgQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8URXWS49Dhmc"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6rPbJE6EPbZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X_iris, y_iris = load_iris(return_X_y=True)\n",
        "# Keep only two classes for this exercise.\n",
        "X_iris = X_iris[y_iris <= 1]\n",
        "y_iris = y_iris[y_iris <= 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APsoy2dQHceV"
      },
      "source": [
        "**Exercise 3.** Make a function that computes\n",
        "$$\n",
        "\\text{softplus}(u) = \\log(1 + e^u)\n",
        "$$\n",
        "and notice that its derivative is\n",
        "$$\n",
        "(\\text{softplus}(u))' = \\frac{e^u}{1 + e^u} = \\frac{1}{1 + e^{-u}} = \\text{sigmoid}(u).\n",
        "$$\n",
        "Using the finite difference formula $f'(u) \\approx \\frac{f(u + \\epsilon) - f(u)}{\\epsilon}$ where epsilon is small value (e.g. 10^-6), check that the derivative of softplus is indeed the sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQGEYam4IYSc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def softplus(u):\n",
        "  # Write your function here\n",
        "  return\n",
        "\n",
        "def finite_difference(f,u):\n",
        "  # Write your function here\n",
        "  return\n",
        "\n",
        "print(softplus(3))\n",
        "print(finite_difference(softplus, 3))\n",
        "print(sigmoid(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230EfSR_Elw0"
      },
      "source": [
        "**Exercise 4.**\n",
        "Make a function that computes the likelihood\n",
        "$$\n",
        "\\text{likelihood}(u_i, y_i) = y_i \\log \\text{sigmoid}(u_i) + (1-y_i) \\log (1-\\text{sigmoid}(u_i))\n",
        "$$\n",
        "where $u_i = \\mathbf{w}^\\top \\mathbf{x}_i$.\n",
        "Using\n",
        "$$\n",
        "\\log \\text{sigmoid}(u) = -\\text{softplus}(-u)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\log(1 - \\text{sigmoid}(u)) = -\\text{softplus}(u)\n",
        "$$\n",
        "make a function that computes the derivative of $\\text{likelihood}(u_i, y_i)$ with respect to $u_i$. Check the result by finite difference. Be careful of signs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj7jUsRhS7BO"
      },
      "outputs": [],
      "source": [
        "def likelihood(u_i, y_i):\n",
        "  # Write function here\n",
        "  return\n",
        "\n",
        "def likelihood_derivative(u_i, y_i):\n",
        "  # Write function here\n",
        "  return\n",
        "\n",
        "print(likelihood_derivative(3, 1))\n",
        "print(finite_difference(likelihood, 3, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BS3nztaQ9q6"
      },
      "source": [
        "**Exercise 5.** Write a function that implements the penalized objective function\n",
        "$$\n",
        "L(\\mathbf{w})\n",
        "= -\\sum_{i=1}^n y_i \\log \\text{sigmoid}(\\mathbf{w}^\\top \\mathbf{x}_i) + (1-y_i) \\log (1-\\text{sigmoid}(\\mathbf{w}^\\top \\mathbf{x}_i)) + \\frac{\\alpha}{2} \\|\\mathbf{w}\\|^2\n",
        "$$\n",
        "and another function that computes its gradient. Reuse `likelihood(u_i, y_i)` and `likelihood_derivative(u_i, y_i)` (you can use a for loop). Check that the gradient is correct using finite differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvH8ea1RHbY7"
      },
      "outputs": [],
      "source": [
        "def objective_value(w, X, y, alpha):\n",
        "  # Write function here\n",
        "  return\n",
        "\n",
        "def objective_gradient(w, X, y, alpha):\n",
        "  # Write function here\n",
        "  return\n",
        "\n",
        "def finite_difference_gradient(func, w, *args, eps=1e-6):\n",
        "  gradient = np.zeros_like(w)\n",
        "  for j in range(len(w)):\n",
        "    e_j = np.zeros(len(w))\n",
        "    e_j[j] = 1\n",
        "    gradient[j] = (func(w + eps * e_j, *args) - func(w, *args)) / eps\n",
        "  return gradient\n",
        "\n",
        "n_samples, n_features = X_iris.shape\n",
        "w = np.random.randn(n_features)\n",
        "alpha = 0.1\n",
        "print(objective_gradient(w, X_iris, y_iris, alpha))\n",
        "print(finite_difference_gradient(objective_value, w, X_iris, y_iris, alpha))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcG3xB4uc3Q0"
      },
      "source": [
        "**Exercise 6.** Implement gradient descent. Check that the objective value is decreasing. Plot the objective value as a function of the number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgYmz3W5dBes"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(value_function, gradient_function, w_init, *args,\n",
        "                     step_size=1e-4, num_iterations=1000):\n",
        "\n",
        "  values = []\n",
        "  w = w_init\n",
        "  # Write gradient descent iteration here.\n",
        "  return values\n",
        "\n",
        "n_samples, n_features = X_iris.shape\n",
        "w_init = np.random.randn(n_features)\n",
        "values = gradient_descent(objective_value, objective_gradient, w, X_iris, y_iris, alpha)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(values)), values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKshmyVEf5At"
      },
      "source": [
        "**Exercise 7.** Rewrite `objective_value` and `objective_gradient` without for loop. Check the correctness of your implementation against the version with a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtIL0DQLgnh9"
      },
      "outputs": [],
      "source": [
        "def objective_value_no_loop(w, X, y, alpha):\n",
        "  # Write your code here\n",
        "  return\n",
        "\n",
        "def objective_gradient_no_loop(w, X, y, alpha):\n",
        "  # Write your code here\n",
        "  return\n",
        "\n",
        "n_samples, n_features = X_iris.shape\n",
        "w = np.random.randn(n_features)\n",
        "alpha = 0.1\n",
        "print(objective_value(w, X_iris, y_iris, alpha))\n",
        "print(objective_value_no_loop(w, X_iris, y_iris, alpha))\n",
        "print(objective_gradient(w, X_iris, y_iris, alpha))\n",
        "print(objective_gradient_no_loop(w, X_iris, y_iris, alpha))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gdsqbPEiroi"
      },
      "source": [
        "Time the two implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqHQJjA8iv05"
      },
      "outputs": [],
      "source": [
        "%time objective_value(w, X_iris, y_iris, alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIWzhYOyi0df"
      },
      "outputs": [],
      "source": [
        "%time objective_value_no_loop(w, X_iris, y_iris, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWxXpj_riZp"
      },
      "source": [
        "# Lecture 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4VsRice8VqR"
      },
      "source": [
        "# Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HoXxpkV8VqR"
      },
      "source": [
        "We will focus on the California dataset and ridge regression. We will start by scaling the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpLM1kao8VqR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = StandardScaler().fit_transform(X_california)\n",
        "y = y_california - y_california.mean()\n",
        "y /= np.std(y_california)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfainsjR8VqT"
      },
      "source": [
        "**Exercise 8.** Write a function that computes the stochastic gradient of ridge regression\n",
        "$$\n",
        "L(\\mathbf{w}) = \\frac{1}{n} \\|\\mathbf{y} - \\mathbf{X} \\mathbf{w}\\|^2\n",
        "+ \\frac{\\alpha}{2} \\|\\mathbf{w}\\|^2\n",
        "$$\n",
        "(notice the 1/n factor).\n",
        "\n",
        "Check that the mean of the stochastic gradients gives the gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyYNDeCw8VqT"
      },
      "outputs": [],
      "source": [
        "def ridge_objective(w, X, y, alpha):\n",
        "  # Write your code here\n",
        "  return\n",
        "\n",
        "\n",
        "def ridge_gradient(w, X, y, alpha):\n",
        "  # Write your code here\n",
        "  return\n",
        "\n",
        "def stochastic_gradient(w, i, X, y, alpha):\n",
        "  # Write your code here\n",
        "  return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIyIKxau8VqV"
      },
      "source": [
        "**Exercise 9.** Write a function that implements stochastic gradient descent. Implement two rules for sampling the index: cyclic, and at random. Compare the convergence of both algorithms. What is the role of the step size?\n",
        "\n",
        "You should especially look at the convergence speed and the value at which the algorithm plateaus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkcj3zn18VqW"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(value_function, gradient_function, w_init, idx_list, *args,\n",
        "                                step_size=1e-4, num_iterations=1000):\n",
        "\n",
        "  values = []\n",
        "  w = w_init\n",
        "  # Write SGD code here\n",
        "  return values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXAGCMLH8VqX"
      },
      "source": [
        "# L-BFGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrQ7waY8VqY"
      },
      "source": [
        "L-BFGS is the go-to second order method. It is already implemented in `scipy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-4jfEsV8VqY"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import fmin_l_bfgs_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuaI7Gpj8Vqa"
      },
      "source": [
        "**Exercise 10.** Use the L-BFGS code to optimize the logistic regression on the Iris dataset. Compare it with your gradient descent.\n",
        "\n",
        "Hint: in order to store the function values, you can use the callback function in `fmin_l_bfgs_b`. Don't forget to read the documentation !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcGjCq098Vqa"
      },
      "outputs": [],
      "source": [
        "class callback(object):\n",
        "    def __init__(self):\n",
        "        self.values = []\n",
        "\n",
        "    def __call__(self, w):\n",
        "        self.values.append(objective_value_no_loop(w, X_iris, y_iris, alpha))\n",
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Optimization lab work",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "52cac47bb829c37db794c82c83a79339992d5d07f5620aa9f303ee48516a5585"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}