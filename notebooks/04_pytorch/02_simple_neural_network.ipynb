{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network\n",
    "\n",
    "\n",
    "In this notebook, we are going to create and train a simple neural network on the digits dataset using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the data and make them into pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Normalize\n",
    "\n",
    "X -= X.mean(axis=0)\n",
    "X /= np.std(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "f, axes = plt.subplots(1, 3)\n",
    "for i, axe in enumerate(axes):\n",
    "    axe.imshow(X[i].reshape(8, 8))\n",
    "\n",
    "x = torch.tensor(X_train).float()\n",
    "y = torch.tensor(y_train).long()\n",
    "n, p = x.shape\n",
    "x_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network\n",
    "\n",
    "We will work with a simple network with two layers (one hidden layer).\n",
    "\n",
    "The input $x$ is transformed into the output $z$ by the following operations:\n",
    "\n",
    "$$y = \\tanh(W_1x + b_1)$$\n",
    "$$z = W_2y + b_2$$\n",
    "\n",
    "**Exercise 1**: Define a function `net(x, W1, b1, W2, b2)` that implements this transform. Remember that `x` is a matrix of size $n\\times p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x, W1, b1, W2, b2):\n",
    "    #Your code here\n",
    "    Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us specify the parameters of the network, `W1, b1, W2, b2`. You can chose the size of the hidden layer, but the input and output sizes are determined by the problem.\n",
    "\n",
    "**Exercise 2**: Define a set of parameters `W1, b1, W2, b2`, where you chose the size of the hidden layer. Make sure that all these parameters have their `requires_grad` flag set to true, so that we can compute the gradient with respect to them.\n",
    "\n",
    "In order to check that eveything works, compute `net(x, W1, b1, W2, b2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size =\n",
    "input_size =\n",
    "output_size =\n",
    "\n",
    "W1 =\n",
    "b1 = \n",
    "W2 =\n",
    "b2 = \n",
    "output = net(x, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a cost function. We will use the classical cross entropy loss. It is imported from pytorch in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Compute the current loss of the network, and then back-propagate to compute the gradient with respect to the parameters. Check the gradient with respect to W1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to train our network!\n",
    "\n",
    "But first, we will need to compute the accuracy of the network, on the train and test set.\n",
    "\n",
    "**Exercise 4**: Define a function `accuracy(X, y, W1, b1, W2, b2)` that computes the accuracy of the network on the dataset `x`with true labels `y`. Remember that the predicted class at the output of the network is computed as the argmaximum of the output. Compute the current accuracy of the network on the train set. Is it normal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(X, y, W1, b1, W2, b2):\n",
    "    # Your code here\n",
    "    return\n",
    "\n",
    "\n",
    "accuracy(x, y, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We are now ready to train the network, using back-propagation and stochastic gradient descent.\n",
    "First, we define the number of iterations of the algorithm, the step size, and the batch size. We also reinitialize the weights. Finally, we will store the train and test accuracy during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "step_size = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "test_list = []\n",
    "train_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Complete the following training list, so that each parameter is updated at each iteration.\n",
    "\n",
    "Remember that at each iteration, you should:\n",
    "* compute the output of the network with respect to the batch\n",
    "* Compute the loss, and backpropagate\n",
    "* Update each parameter with gradient descent\n",
    "* Refresh the gradient of each parameter. To do so, you can do:\n",
    "\n",
    "```\n",
    "W1 = W1.detach()\n",
    "W1.requires_grad=True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_iter):\n",
    "    # Selection of the batch here\n",
    "    batch_idx = torch.argsort(torch.randn(n))[:batch_size]\n",
    "    x_batch = x[batch_idx]\n",
    "    y_batch = y[batch_idx]\n",
    "    # YOUR CODE HERE: Compute the output of the network, the loss, and backpropagate\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # YOUR CODE HERE: update each parameter, and refresh the gradients.\n",
    "    # Utility to print the current state of training\n",
    "    if i % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            train_acc = accuracy(x, y, W1, b1, W2, b2)\n",
    "            test_acc = accuracy(x_test, y_test, W1, b1, W2, b2)\n",
    "        test_list.append(test_acc)\n",
    "        train_list.append(train_acc)\n",
    "        print('Iteration {} Train loss: {:1.3f} Train acc: {:1.3f} Test acc {:1.3f}'.format(i, loss.item(), train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**: Display the learning curves. You can then play with the network and training parameters:\n",
    "what happens when you change the learning rate, the number of hidden sizes, etc?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
